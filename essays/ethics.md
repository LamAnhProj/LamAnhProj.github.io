---
layout: essay
type: essay
title: "Are We Ready for Utilitarian Cars?"
date: 2017-05-02
labels:
  - Software Engineering
  - Ethics
---

## Are We Ready for Utilitarian Cars?
<iimg class="ui floated medium left image" src"https://d267cvn3rvuq91.cloudfront.net/i/images/Ethical%20cars.png?sw=1080">

Being a junior here at Manoa, something that I have notice about my degree so far, is the almost complete lack of mention of any ethical considerations behind the things we learn about. I feel like many jobs that computer science graduates go on to do involve situations where ethics can be really relevant. For example, thinking about the kinds of privacy issues that come up with the work that Facebook and Google do, and even the NSA and GCHQ -- many of the people who implement those systems have computer science degrees. Ethics in software engineering has been defined as the application of both computer science and engineering philosophy, principles, and practices to the design and development of software systems. Due to this engineering focus and the increased use of software in mission critical and human critical systems, where failure can result in large losses of capital but more importantly lives, many ethical codes have been developed by a number of societies, associations and organizations. There are also issues in academic and research computer science: think about drone research, or other military-funded work. Not everyone will have a problem with doing research funded by the army, but it's something that people should be aware of and think about, so if it might bother them, they can have those conversations and ask the right questions before they end up in a situation they are uncomfortable with.

In 2007, a sequence of technical advances enabled six teams to complete the DARPA Urban Challenge, the first benchmark test for autonomous driving in realistic urban environments.

Not discouraging buyers is a commercial necessity—but it is also in itself a moral imperative, given the social and safety benefits AVs provide over conventional cars. Meanwhile, avoiding public outrage, that is, adopting moral algorithms that align with human moral attitudes, is key to fostering public comfort with allowing the broad use of AVs in the first place. However, to pursue these two objectives simultaneously may lead to moral inconsistencies. Consider for example the case displayed in Fig. 1a, and assume that the most common moral attitude is that the car should swerve. This would fit a utilitarian moral doctrine [Rosen, 2005], according to which
the moral course of action is to minimize the death toll. But consider then the case displayed in Fig. 1c. The utilitarian course of action, in that situation, would be for the car to swerve and kill its owner—but a driverless car programmed. to follow this course of action might discourage buyers, who may consider that their own safety should trump other considerations. 

Results suggest that participants were generally comfortable with utilitarian AVs, programmed to minimize an accident’s death toll. This is especially true of situations that do not involve the sacrifice of the AV’s owner.
