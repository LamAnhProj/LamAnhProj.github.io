---
layout: essay
type: essay
title: "Are We Ready for Utilitarian Cars?"
date: 2017-05-02
labels:
  - Software Engineering
  - Ethics
---


<img class="ui floated medium left image" src"https://d267cvn3rvuq91.cloudfront.net/i/images/Ethical%20cars.png?sw=1080">

Being a junior here at Manoa, something that I have notice about my degree so far, is the almost complete lack of mention of any ethical considerations behind the things we learn about. I feel like many jobs that computer science graduates go on to do involve situations where ethics can be really relevant. For example, thinking about the kinds of privacy issues that come up with the work that Facebook and Google do, and even the NSA and GCHQ -- many of the people who implement those systems have computer science degrees. Ethics in software engineering has been defined as the application of both computer science and engineering philosophy, principles, and practices to the design and development of software systems. Due to this engineering focus and the increased use of software in mission critical and human critical systems, where failure can result in large losses of capital but more importantly lives, many ethical codes have been developed by a number of societies, associations and organizations. There are also issues in academic and research computer science: think about drone research, or other military-funded work. Not everyone will have a problem with doing research funded by the army, but it's something that people should be aware of and think about, so if it might bother them, they can have those conversations and ask the right questions before they end up in a situation they are uncomfortable with. The question of ethics and how far to trace cause have always been tricky. Are gun manufacturers responsible for gun violence? If they are, what about part manufacturers? If they are, what about metals vendors? If they are, what about the mining operations?
The conversation could equally be about car manufacturers and pollution, or unintended uses (using a car as a weapon, running someone down).
Software developers tend to be somewhere in the middle of the layers of causation (and IMHO, ethical accountability). They tend to work in profoundly complex environments with many layers and many other human beings. As an OS developer, my OS could have evil stuff run on it. As a library developer, my stuff could end up in unethical applications. As As a app developer, my stuff could be used in an unintended manner, or by people I didn't intend to ever use it. Am I accountable for any of these things, and to what degree?

In 2007, a sequence of technical advances enabled six teams to complete the DARPA Urban Challenge, the first benchmark test for autonomous driving in realistic urban environments. The answers to these ethical questions are important because they could have a big impact on the way self-driving cars are accepted in society. Who would buy a car programmed to sacrifice the owner?


Not discouraging buyers is a commercial necessity—but it is also in itself a moral imperative, given the social and safety benefits AVs provide over conventional cars. Meanwhile, avoiding public outrage, that is, adopting moral algorithms that align with human moral attitudes, is key to fostering public comfort with allowing the broad use of AVs in the first place. However, to pursue these two objectives simultaneously may lead to moral inconsistencies.  The utilitarian course of action, in that situation, would be for the car to swerve and kill its owner—but a driverless car programmed. to follow this course of action might discourage buyers, who may consider that their own safety should trump other considerations. 

Results suggest that participants were generally comfortable with utilitarian AVs, programmed to minimize an accident’s death toll. This is especially true of situations that do not involve the sacrifice of the AV’s owner.
